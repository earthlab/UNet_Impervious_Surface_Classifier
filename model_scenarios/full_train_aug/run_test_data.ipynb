{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "# import cv2\n",
    "import rasterio as rio\n",
    "\n",
    "rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'medium',\n",
    "        'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "import os,sys\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import fiona\n",
    "from shapely.geometry import shape\n",
    "import shapely\n",
    "from rasterio.mask import mask\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "import rasterio\n",
    "\n",
    "# add the unet helpers\n",
    "sys.path.append('../../')\n",
    "from test_unet_helpers import *\n",
    "from unet_models import unet11_MS\n",
    "\n",
    "# torch stuff\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torchvision import models, datasets\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from utils import variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_classes=2, depth=5, wf=6, padding=False,\n",
    "                 batch_norm=False, up_mode='upconv'):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(UNetConvBlock(prev_channels, 2**(wf+i),\n",
    "                                                padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(UNetUpBlock(prev_channels, 2**(wf+i), up_mode,\n",
    "                                            padding, batch_norm))\n",
    "            prev_channels = 2**(wf+i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path)-1:\n",
    "                blocks.append(x)\n",
    "                x = F.avg_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i-1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3,\n",
    "                               padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2,\n",
    "                                         stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                                    nn.Conv2d(in_size, out_size, kernel_size=1))\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[:, :, diff_y:(diff_y + target_size[0]), diff_x:(diff_x + target_size[1])]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "from glob import glob\n",
    "class DG_GT_Dataset(Dataset):\n",
    "    \"\"\"Dataset class for ignition types (Y var)\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, channels='all', img_transform=None, gt_transform=None):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            data_dir (string): the folder containing the image files\n",
    "            channels (string): 'all', 'bgr', 'bgrn1', 'bgrn2'; band sets for DG imagery\n",
    "            img_transform (callable, optional): Optional transform to  be applied to source image data\n",
    "            gt_transform (callable, optional): Optional transform to  be applied to labeled image data\n",
    "            x_var (iterable, optional): list of predictor variable names\n",
    "            land_mask (string, optional): defines whether or not to return land mask\n",
    "        \"\"\"\n",
    "        \n",
    "        # some sanity checks\n",
    "        assert os.path.exists(data_dir)\n",
    "        \n",
    "        self.gt_files = sorted(glob(data_dir + '/gt*.tif'))\n",
    "        self.img_files = sorted(glob(data_dir + '/dg*.tif'))\n",
    "        \n",
    "        \n",
    "        print(self.img_files[0])\n",
    "        print(self.gt_files[0])\n",
    "        \n",
    "        self.img_transform = img_transform\n",
    "        self.gt_transform = gt_transform\n",
    "        self.dg_bgr = [1,2,4]\n",
    "        self.dg_bgrn1 = [1,2,4,6]\n",
    "        self.dg_bgrn2 = [1,2,4,7]\n",
    "        self.channels = channels\n",
    "        \n",
    "        assert len(self.img_files) == len(self.gt_files)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \"\"\"\n",
    "            Files are organized as <var_type>_<year>_<month>_t<tileNumber>.tif, e.g., Arson_1992_1_t1\n",
    "            A single dataset needs to be constructed for a given ignition type, year, month, and tile number\n",
    "        \"\"\"\n",
    "        \n",
    "        img_file = self.img_files[idx]\n",
    "        gt_file = self.gt_files[idx]\n",
    "        \n",
    "        with rio.open(img_file) as src:\n",
    "            img_arr = src.read()\n",
    "            \n",
    "        # check the channels\n",
    "        if self.channels == 'bgr':\n",
    "            img_arr = img_arr[self.dg_bgr, :, :]\n",
    "        elif self.channels == 'bgrn1':\n",
    "            img_arr = img_arr[self.dg_bgrn1, :, :]\n",
    "        elif self.channels == 'bgrn2':\n",
    "            img_arr = img_arr[self.dg_brgn2, :, :]\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        with rio.open(gt_file) as src:\n",
    "            gt_arr = src.read()\n",
    "\n",
    "        if (self.img_transform is not None):\n",
    "            return (self.img_transform(torch.from_numpy(img_arr)), \n",
    "                    self.gt_transform(torch.from_numpy(gt_arr)))   \n",
    "        else:\n",
    "            return (torch.from_numpy(img_arr), torch.from_numpy(gt_arr)) # return X, Y, Mask (Mask uses LandMask in X-var folder)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that data is ready, set up the model and run through it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some code for setting up the model and performance eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/spatial_torch/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import utils as pyt_utils\n",
    "from torch.optim import Adam\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pathlib import Path\n",
    "from validation import validation_binary\n",
    "from loss import LossBinary\n",
    "import json\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# need to change this to DICE loss!\n",
    "#loss = LossBinary(jaccard_weight=args.jaccard_weight)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO the same for pansharpened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../tiles/kmeans3_split/tilesPS_d10000_512x512/train/dg_is_00000.tif\n",
      "../../../tiles/kmeans3_split/tilesPS_d10000_512x512/train/gt_is_00000.tif\n",
      "../../../tiles/kmeans3_split/tilesPS_d10000_512x512/test/dg_is_00731.tif\n",
      "../../../tiles/kmeans3_split/tilesPS_d10000_512x512/test/gt_is_00731.tif\n",
      "../../../tiles/kmeans3_split/tilesPS_d10000_512x512/val/dg_is_00500.tif\n",
      "../../../tiles/kmeans3_split/tilesPS_d10000_512x512/val/gt_is_00500.tif\n"
     ]
    }
   ],
   "source": [
    "train_folder = r'D:\\projects\\RD\\debug_IS_segmentation\\tilesPS_d10000_512x512\\train'\n",
    "test_folder = r'D:\\projects\\RD\\debug_IS_segmentation\\tilesPS_d10000_512x512\\test'\n",
    "val_folder = r'D:\\projects\\RD\\debug_IS_segmentation\\tilesPS_d10000_512x512\\val'\n",
    "\n",
    "# ubuntu paths\n",
    "train_folder = '/media/joemcglinchy/Data/projects/RD/debug_IS_segmentation/kmeans3_split/tilesPS_d10000_512x512/train'\n",
    "test_folder = '/media/joemcglinchy/Data/projects/RD/debug_IS_segmentation/kmeans3_split/tilesPS_d10000_512x512/test'\n",
    "val_folder = '/media/joemcglinchy/Data/projects/RD/debug_IS_segmentation/kmeans3_split/tilesPS_d10000_512x512/val'\n",
    "\n",
    "# aws pathts\n",
    "train_folder = '../../../tiles/kmeans3_split/tilesPS_d10000_512x512/train'\n",
    "test_folder = '../../../tiles/kmeans3_split/tilesPS_d10000_512x512/test'\n",
    "val_folder = '../../../tiles/kmeans3_split/tilesPS_d10000_512x512/val'\n",
    "\n",
    "# load as 4 band\n",
    "ps_train_ds = DG_GT_Dataset(train_folder, channels='bgrn1')\n",
    "ps_test_ds = DG_GT_Dataset(test_folder, channels='bgrn1')\n",
    "ps_val_ds = DG_GT_Dataset(val_folder, channels='bgrn1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_bands is  4\n",
      "Restored model, epoch files_MS_aug_bgrn1/bgrn1_ms_aug_ep500_step16000_b32.pt, step 16,000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet11_MS(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (relu): ReLU(inplace)\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3s): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4s): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5s): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (center): DecoderBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ConvRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU(inplace)\n",
       "      )\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (dec5): DecoderBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ConvRelu(\n",
       "        (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU(inplace)\n",
       "      )\n",
       "      (1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (dec4): DecoderBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ConvRelu(\n",
       "        (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU(inplace)\n",
       "      )\n",
       "      (1): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (dec3): DecoderBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ConvRelu(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU(inplace)\n",
       "      )\n",
       "      (1): ConvTranspose2d(256, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (dec2): DecoderBlock(\n",
       "    (block): Sequential(\n",
       "      (0): ConvRelu(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU(inplace)\n",
       "      )\n",
       "      (1): ConvTranspose2d(128, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "      (2): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (dec1): ConvRelu(\n",
       "    (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (activation): ReLU(inplace)\n",
       "  )\n",
       "  (final): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RGB-NIR\n",
    "#model_path = './files_PS_bgrn1/bgrn1_ps_ep75_step1575_b24.pt'\n",
    "model_path = './files_MS_aug_bgrn1/bgrn1_ms_aug_ep500_step16000_b32.pt'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#model = unet11(pretrained=False) # B-G-R\n",
    "ps_model = unet11_MS(num_bands=4, pretrained=False) \n",
    "\n",
    "\n",
    "# load on CPU\n",
    "if os.path.exists(model_path):\n",
    "    state_dict = torch.load(str(model_path), map_location='cpu')\n",
    "    epoch = state_dict['epoch']\n",
    "    step = state_dict['step']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict['model'].items():\n",
    "        name = k[7:] # remove 'module.' of dataparallel\n",
    "        new_state_dict[name]=v\n",
    "\n",
    "    ps_model.load_state_dict(new_state_dict)\n",
    "    print('Restored model, epoch {}, step {:,}'.format(epoch, step))\n",
    "\n",
    "ps_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tps = []\n",
    "all_fps= []\n",
    "maxes = []\n",
    "mins = []\n",
    "aucs = []\n",
    "for ind in range(len(ps_val_ds)):\n",
    "    test_im, test_target = ps_val_ds[ind]\n",
    "    test_out = ps_model(variable(test_im.unsqueeze(0)))\n",
    "    \n",
    "\n",
    "    out = test_out.cpu().detach().numpy()[0][0]\n",
    "    maxes.append(out.max())\n",
    "    mins.append(out.min())\n",
    "    \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(test_target.numpy().flatten(), out.flatten())\n",
    "    aucs.append(roc_auc_score(test_target.numpy().flatten(), out.flatten()))\n",
    "    all_tps.append(true_positive_rate)\n",
    "    all_fps.append(false_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'weight' : 'medium',\n",
    "        'size'   : 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(len(all_tps)):\n",
    "    \n",
    "    plt.plot(all_fps[i], all_tps[i])\n",
    "    \n",
    "plt.plot([0,1], [0,1], 'r--')\n",
    "plt.title('ROC Curves for Test Set: Blue-Green-Red-NIR1 bands')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(maxes), min(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasters = glob('./rastertiles2015/*.tif')\n",
    "rasmax2, rasmin2 = [], []\n",
    "for f in rasters:\n",
    "    with rio.open(f) as src:\n",
    "        arr = src.read()\n",
    "        rasmax2.append(arr.max())\n",
    "        rasmin2.append(arr.min())\n",
    "        \n",
    "rasters2 = glob('./rastertiles2/*.tif')\n",
    "rasmax, rasmin = [], []\n",
    "for f in rasters2:\n",
    "    with rio.open(f) as src:\n",
    "        arr = src.read()\n",
    "        rasmax.append(arr.max())\n",
    "        rasmin.append(arr.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(rasmax2), min(rasmin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(maxes, alpha=0.3, bins=100, label='model data')\n",
    "plt.hist(rasmax2, alpha=0.3, bins=100, label='2015 raster data')\n",
    "plt.hist(rasmax, alpha=0.3, bins=100, label='2016 raster data')\n",
    "plt.legend()\n",
    "plt.xlabel('Image Tile Maximum')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(aucs, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(aucs), min(aucs), max(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_spatial_torch)",
   "language": "python",
   "name": "conda_spatial_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
